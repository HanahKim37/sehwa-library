# pages/02_ë…ì„œê°€.py

import re
from io import BytesIO
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from difflib import SequenceMatcher

import pandas as pd
import streamlit as st
from openpyxl import Workbook
from openpyxl.styles import Alignment, Font

# =========================================================
# 0) ì„ê³„ê°’
# =========================================================
REQUIRED_TITLE_THRESHOLD = 0.80
REQUIRED_AUTHOR_THRESHOLD = 0.60
DUP_TITLE_THRESHOLD = 0.80
DUP_AUTHOR_THRESHOLD = 0.60

# =========================================================
# 1) í˜ì´ì§€ ì„¤ì •
# =========================================================
APP_TITLE = "ğŸ“š ë…ì„œê°€"
st.set_page_config(page_title="ë…ì„œê°€", page_icon="ğŸ“š", layout="wide")
st.title(APP_TITLE)
st.caption("ë…ì„œí™œë™ìƒí™© ì—‘ì…€ì„ ì—…ë¡œë“œí•˜ë©´ í•™ê¸°ë³„ ì¶©ì¡± ì—¬ë¶€ì™€ ì´ ì¶©ì¡± ì—¬ë¶€ë¥¼ ì‚°ì¶œí•©ë‹ˆë‹¤.")

st.info(
    "â€» ë…ì„œí™œë™ìƒí™© ì—‘ì…€ì€ **í•œ ë°˜ì˜ íŒŒì¼ë§Œ** ì—…ë¡œë“œí•´ ì£¼ì„¸ìš”.\n\n"
    "âœ… ë˜í•œ **íŒŒì¼ëª…ì— í•™ë…„-ë°˜ì„ ë°˜ë“œì‹œ í¬í•¨**í•´ ì£¼ì„¸ìš”. (ì˜ˆ: `1-1`, `3-12`)\n"
    "- ì˜ˆì‹œ íŒŒì¼ëª…: `ë…ì„œí™œë™_2-10.xlsx`, `2-10_ë…ì„œí™œë™ìƒí™©.xlsx`"
)

# =========================================================
# 2) ë‚´ì¥ í•„ë…ì„œ ê²½ë¡œ
# =========================================================
PROJECT_ROOT = Path(__file__).resolve().parents[1]
REQUIRED_2024_PATH = PROJECT_ROOT / "data" / "required_books" / "í•„ë…ì„œ_2024.xlsx"
REQUIRED_2025_PATH = PROJECT_ROOT / "data" / "required_books" / "í•„ë…ì„œ_2025.xlsx"

# =========================================================
# 3) ìœ í‹¸
# =========================================================
def _normalize_text(s: str) -> str:
    if s is None:
        return ""
    s = str(s).strip().lower()
    s = re.sub(r"\s+", "", s)
    s = re.sub(r"[^\wê°€-í£]", "", s)
    return s


def _title_variants_norm(title: str) -> List[str]:
    if title is None:
        return []
    t = str(title).strip()
    if not t:
        return []
    seps = ["(", "ï¼ˆ", "[", "ã€", ":", "ï¼š", "/", "ï¼", " - ", " â€“ ", " â€” ", "-", "â€“", "â€”"]
    candidates = [t]
    for sep in seps:
        if sep in t:
            candidates.append(t.split(sep, 1)[0].strip())

    out: List[str] = []
    for c in candidates:
        n = _normalize_text(c)
        if n and n not in out:
            out.append(n)
    return out


def _author_variants_norm(author: str) -> List[str]:
    if author is None:
        return []
    a = str(author).strip()
    if not a:
        return []
    tokens = [t for t in re.split(r"\s+", a) if t.strip()]
    out: List[str] = []

    full = _normalize_text(a)
    if full:
        out.append(full)

    if tokens:
        last = tokens[-1]
        last_n = _normalize_text(last)
        if last_n and last_n not in out:
            out.append(last_n)

        initials = []
        for t in tokens[:-1]:
            t2 = re.sub(r"[^A-Za-zê°€-í£]", "", t)
            if t2:
                initials.append(t2[0].lower())
        if initials and last_n:
            combo = _normalize_text("".join(initials) + last)
            if combo and combo not in out:
                out.append(combo)

    return out


def _similarity(a: str, b: str) -> float:
    return SequenceMatcher(None, a, b).ratio()


def _best_title_similarity(title_a: str, title_b: str) -> float:
    va = _title_variants_norm(title_a)
    vb = _title_variants_norm(title_b)
    best = 0.0
    for x in va:
        for y in vb:
            if len(x) < 3 or len(y) < 3:
                continue
            best = max(best, _similarity(x, y))
    return best


def _best_author_similarity(author_a: str, author_b: str) -> float:
    va = _author_variants_norm(author_a)
    vb = _author_variants_norm(author_b)
    best = 0.0
    for x in va:
        for y in vb:
            if len(x) < 3 or len(y) < 3:
                continue
            if (len(x) >= 4 and x in y) or (len(y) >= 4 and y in x):
                best = max(best, 1.0)
            else:
                best = max(best, _similarity(x, y))
    return best


def _pick_col(df: pd.DataFrame, candidates: List[str]) -> Optional[str]:
    cols = list(df.columns)
    for cand in candidates:
        for col in cols:
            if col is None:
                continue
            if cand in str(col).replace(" ", ""):
                return col
    return None


def _extract_grade_class_from_filename_with_raw(filename: str) -> Tuple[Optional[int], Optional[int], Optional[str]]:
    if not filename:
        return None, None, None
    m = re.search(r"(?<!\d)([1-3])\s*[-_ ]\s*(\d{1,2})(?!\d)", filename)
    if not m:
        return None, None, None
    g = int(m.group(1))
    c_str = m.group(2)
    c = int(c_str)
    if g not in [1, 2, 3]:
        return None, None, None
    if not (1 <= c <= 12):
        return None, None, None
    raw = f"{g}-{c_str}"
    return g, c, raw


def _parse_grade_class_from_sheet_top(df_raw: pd.DataFrame) -> Tuple[Optional[int], Optional[int]]:
    max_r = min(20, df_raw.shape[0])
    max_c = min(10, df_raw.shape[1])
    pattern = re.compile(r"(\d)\s*í•™\s*ë…„\D{0,10}?(\d{1,2})\s*ë°˜")
    for r in range(max_r):
        for c in range(max_c):
            v = df_raw.iat[r, c]
            if pd.isna(v):
                continue
            m = pattern.search(str(v))
            if not m:
                continue
            grade = int(m.group(1))
            cls = int(m.group(2))
            if grade in [1, 2, 3] and cls >= 1:
                return grade, cls
    return None, None


def _infer_academic_year_from_print_date(df_raw_top: pd.DataFrame) -> Optional[int]:
    date_pat = re.compile(r"(20\d{2})[.\-/]\s*(\d{1,2})[.\-/]\s*(\d{1,2})")
    max_r = min(30, df_raw_top.shape[0])
    max_c = min(20, df_raw_top.shape[1])
    for r in range(max_r):
        for c in range(max_c):
            v = df_raw_top.iat[r, c]
            if pd.isna(v):
                continue
            m = date_pat.search(str(v))
            if m:
                y = int(m.group(1))
                mo = int(m.group(2))
                return (y - 1) if mo in [1, 2] else y
    return None


def _find_header_row_for_reading(df_raw: pd.DataFrame) -> Optional[int]:
    for r in range(min(80, df_raw.shape[0])):
        row_vals = [str(x).strip() for x in df_raw.iloc[r].tolist() if not pd.isna(x)]
        joined = " ".join(row_vals).replace(" ", "")
        if "ë²ˆí˜¸" in joined and ("ë…ì„œ" in joined or "ë…ì„œí™œë™" in joined or "ë…ì„œí™œë™ìƒí™©" in joined):
            return r
    return None


def _load_reading_table(xls: pd.ExcelFile, sheet_name: str) -> pd.DataFrame:
    df_raw = xls.parse(sheet_name=sheet_name, header=None, dtype=str)
    header_row = _find_header_row_for_reading(df_raw)
    if header_row is None:
        return xls.parse(sheet_name=sheet_name, dtype=str)

    headers = df_raw.iloc[header_row].tolist()
    df = df_raw.iloc[header_row + 1 :].copy()
    df.columns = headers
    df = df.dropna(how="all")
    return df


BOOK_PAIR_RE = re.compile(r"(?P<title>[^()]+?)\s*\(\s*(?P<author>[^()]+?)\s*\)")


def _split_books(cell_text: str) -> List[Tuple[str, str]]:
    if cell_text is None or pd.isna(cell_text):
        return []
    text = str(cell_text).strip()
    if not text:
        return []
    books: List[Tuple[str, str]] = []
    for m in BOOK_PAIR_RE.finditer(text):
        title = m.group("title").strip().rstrip(",").strip()
        author = m.group("author").strip().rstrip(",").strip()
        if title and author:
            books.append((title, author))
    return books


def _semester_sort_key(year: str, sem: str) -> Tuple[int, int]:
    try:
        y = int(re.findall(r"\d+", str(year))[0])
    except Exception:
        y = 9999
    nums = re.findall(r"\d+", str(sem))
    s = int(nums[0]) if nums else 9
    return (y, s)


def _find_req_header_row(df_raw: pd.DataFrame) -> Optional[int]:
    max_r = min(200, df_raw.shape[0])
    for r in range(max_r):
        row = df_raw.iloc[r].astype(str).fillna("").tolist()
        joined = " ".join([x.strip() for x in row if x and x.strip() != "nan"]).replace(" ", "")
        if ("ë„ì„œëª…" in joined) and (("ì €ìëª…" in joined) or ("ì €ì" in joined)):
            return r
    return None


@st.cache_data(show_spinner=False)
def _load_required_books_from_repo(xlsx_path: str) -> Dict[str, Tuple[str, str]]:
    p = Path(xlsx_path)
    if not p.exists():
        raise FileNotFoundError(f"í•„ë…ì„œ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {p.resolve()}")

    xls = pd.ExcelFile(p, engine="openpyxl")
    sheet = xls.sheet_names[0]

    raw = xls.parse(sheet_name=sheet, header=None, dtype=str).dropna(how="all")
    header_row = _find_req_header_row(raw)
    if header_row is None:
        raise ValueError(f"í•„ë…ì„œ íŒŒì¼ì—ì„œ 'ë„ì„œëª…/ì €ì(ì €ìëª…)' í—¤ë” í–‰ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤: {p.resolve()}")

    headers = raw.iloc[header_row].tolist()
    df = raw.iloc[header_row + 1 :].copy()
    df.columns = headers

    title_col = _pick_col(df, ["ë„ì„œëª…", "ë„ì„œ", "ì œëª©"])
    author_col = _pick_col(df, ["ì €ìëª…", "ì €ì", "ì§€ì€ì´", "ì‘ê°€"])
    if not title_col or not author_col:
        raise ValueError(f"í•„ë…ì„œ íŒŒì¼ ì»¬ëŸ¼ ì¸ì‹ ì‹¤íŒ¨: {p.resolve()}")

    out: Dict[str, Tuple[str, str]] = {}
    for _, row in df.iterrows():
        t = row.get(title_col, None)
        a = row.get(author_col, None)
        if t is None or a is None:
            continue
        if pd.isna(t) or pd.isna(a):
            continue

        t = str(t).strip()
        a = str(a).strip()
        if not t or not a:
            continue

        t_ns = t.replace(" ", "")
        a_ns = a.replace(" ", "")
        if t_ns in ["ë„ì„œëª…", "ë„ì„œ", "ì œëª©"] or a_ns in ["ì €ì", "ì €ìëª…", "ì§€ì€ì´"]:
            continue
        if t.lower() == "nan" or a.lower() == "nan":
            continue

        key = _normalize_text(t) + "|" + _normalize_text(a)
        out[key] = (t, a)

    if not out:
        raise ValueError(f"í•„ë…ì„œ ë°ì´í„°ê°€ 0ê±´ìœ¼ë¡œ ì¸ì‹ë˜ì—ˆìŠµë‹ˆë‹¤: {p.resolve()}")

    return out


def _safe_year_int(x) -> Optional[int]:
    try:
        return int(re.findall(r"\d+", str(x))[0])
    except Exception:
        return None


def _choose_required_map(year_value, req_2024_map, req_2025_map):
    y = _safe_year_int(year_value)
    if y == 2024:
        return req_2024_map
    if y is None or y >= 2025:
        return req_2025_map
    return req_2024_map


def _build_required_title_map(req_map: Dict[str, Tuple[str, str]]) -> Dict[str, Tuple[str, str]]:
    title_map: Dict[str, Tuple[str, str]] = {}
    for _, (t, a) in req_map.items():
        for nt in _title_variants_norm(t):
            if nt not in title_map:
                title_map[nt] = (t, a)
    return title_map


def _fuzzy_match_required_title_author(
    raw_title: str,
    raw_author: str,
    req_title_map: Dict[str, Tuple[str, str]],
    title_threshold: float = REQUIRED_TITLE_THRESHOLD,
    author_threshold: float = REQUIRED_AUTHOR_THRESHOLD,
) -> Optional[Tuple[str, Tuple[str, str], float, float]]:
    if not raw_title:
        return None

    best_key = None
    best_title_score = 0.0
    best_std = None

    for k, v in req_title_map.items():
        if not k or len(k) < 3:
            continue
        title_scores = []
        for vt in _title_variants_norm(raw_title):
            if len(vt) < 3:
                continue
            title_scores.append(_similarity(vt, k))
        if not title_scores:
            continue
        score = max(title_scores)
        if score > best_title_score:
            best_title_score = score
            best_key = k
            best_std = v

    if best_key is None or best_std is None or best_title_score < title_threshold:
        return None

    std_t, std_a = best_std
    author_score = _best_author_similarity(raw_author, std_a)
    if author_score < author_threshold:
        return None

    return best_key, (std_t, std_a), best_title_score, author_score


def _find_duplicate_against_seen(
    title: str,
    author: str,
    seen: List[dict],
    title_threshold: float = DUP_TITLE_THRESHOLD,
    author_threshold: float = DUP_AUTHOR_THRESHOLD,
) -> Optional[Tuple[dict, float, float]]:
    best_item = None
    best_title = 0.0
    best_author = 0.0

    for it in seen:
        t2 = it.get("title", "")
        a2 = it.get("author", "")

        ts = _best_title_similarity(title, t2)
        if ts < title_threshold:
            continue

        ascore = _best_author_similarity(author, a2)
        if ascore < author_threshold:
            continue

        if (ts > best_title) or (ts == best_title and ascore > best_author):
            best_item = it
            best_title = ts
            best_author = ascore

    if best_item is None:
        return None
    return best_item, best_title, best_author


# openpyxl rich text ê°€ëŠ¥ ì—¬ë¶€
RICH_TEXT_AVAILABLE = False
try:
    from openpyxl.cell.rich_text import CellRichText, TextBlock, InlineFont  # type: ignore
    RICH_TEXT_AVAILABLE = True
except Exception:
    RICH_TEXT_AVAILABLE = False


def _set_books_cell(ws, row_idx: int, col_idx: int, books: List[Tuple[str, str, bool]]):
    if not books:
        ws.cell(row=row_idx, column=col_idx).value = ""
        return

    cleaned = []
    for (t, a, is_req) in books:
        tt = str(t).strip().rstrip(",").strip()
        aa = str(a).strip().rstrip(",").strip()
        cleaned.append((tt, aa, is_req))

    if RICH_TEXT_AVAILABLE:
        rt = CellRichText()
        for i, (t, a, is_req) in enumerate(cleaned):
            text = f"{t}({a})"
            font = InlineFont(b=True) if is_req else InlineFont(b=False)
            rt.append(TextBlock(font, text))
            if i != len(cleaned) - 1:
                rt.append(TextBlock(InlineFont(b=False), ", "))
        ws.cell(row=row_idx, column=col_idx).value = rt
    else:
        items = []
        for (t, a, is_req) in cleaned:
            prefix = "â˜…" if is_req else ""
            items.append(f"{prefix}{t}({a})")
        ws.cell(row=row_idx, column=col_idx).value = ", ".join(items)


def _sem_num(s: str) -> Optional[int]:
    nums = re.findall(r"\d+", str(s))
    if not nums:
        return None
    n = int(nums[0])
    return n if n in [1, 2] else None


def _row_semkey(row: dict, base_year: Optional[int], next_year: Optional[int]) -> Optional[str]:
    y = _safe_year_int(row.get("í•™ë…„ë„"))
    s = _sem_num(row.get("í•™ê¸°"))
    if y is None or s is None or base_year is None:
        return None
    if y == base_year:
        return f"1{s}"
    if next_year is not None and y == next_year:
        return f"2{s}"
    return None


# =========================================================
# 4) ì—…ë¡œë“œ/ì‹¤í–‰ UI
# =========================================================
reading_file = st.file_uploader("ë…ì„œí™œë™ìƒí™© ì—‘ì…€ ì—…ë¡œë“œ (.xlsx)", type=["xlsx"], key="reading_file")
run = st.button("âœ… ì¶©ì¡± ì—¬ë¶€ íŒë‹¨ ì‹¤í–‰", type="primary", use_container_width=True)

# ì„¸ì…˜ ìƒíƒœ
if "analysis" not in st.session_state:
    st.session_state.analysis = None
if "analysis_id" not in st.session_state:
    st.session_state.analysis_id = None
if "excel_bytes" not in st.session_state:
    st.session_state.excel_bytes = None
if "excel_name" not in st.session_state:
    st.session_state.excel_name = None
if "excel_id" not in st.session_state:
    st.session_state.excel_id = None

current_id = None
if reading_file is not None:
    current_id = f"{getattr(reading_file, 'name', '')}:{getattr(reading_file, 'size', '')}"

# ì—…ë¡œë“œ íŒŒì¼ì´ ë°”ë€Œë©´ ê²°ê³¼ ì´ˆê¸°í™”
if st.session_state.analysis is not None and current_id and st.session_state.analysis_id != current_id:
    st.session_state.analysis = None
    st.session_state.analysis_id = None
    st.session_state.excel_bytes = None
    st.session_state.excel_name = None
    st.session_state.excel_id = None
    for k in ["cb_11", "cb_12", "cb_21", "cb_22", "cb_init_for_id"]:
        if k in st.session_state:
            del st.session_state[k]
    st.info("ì—…ë¡œë“œ íŒŒì¼ì´ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤. ì‹¤í–‰ ë²„íŠ¼ì„ ë‹¤ì‹œ ëˆŒëŸ¬ ë¶„ì„ì„ ê°±ì‹ í•´ ì£¼ì„¸ìš”.")


def _analyze(uploaded) -> Dict[str, object]:
    filename = getattr(uploaded, "name", "")
    grade, cls, gc_text_raw = _extract_grade_class_from_filename_with_raw(filename)

    xls = pd.ExcelFile(uploaded, engine="openpyxl")
    sheet = xls.sheet_names[0]
    df_raw_top = xls.parse(sheet_name=sheet, header=None, dtype=str)

    if grade is None or cls is None:
        g2, c2 = _parse_grade_class_from_sheet_top(df_raw_top)
        grade = grade if grade is not None else g2
        cls = cls if cls is not None else c2
        if grade is not None and cls is not None:
            gc_text_raw = f"{grade}-{cls}"

    gc_text = gc_text_raw if gc_text_raw else "í•™ë…„ë°˜ë¯¸ì¸ì‹"

    req_2024_map = _load_required_books_from_repo(str(REQUIRED_2024_PATH))
    req_2025_map = _load_required_books_from_repo(str(REQUIRED_2025_PATH))

    df = _load_reading_table(xls, sheet)

    num_col = _pick_col(df, ["ë²ˆí˜¸"])
    name_col = _pick_col(df, ["ì„±ëª…", "ì´ë¦„"])
    last_col = _pick_col(df, ["ì„±"])
    first_col = _pick_col(df, ["ëª…"])
    year_col = _pick_col(df, ["í•™ë…„ë„"])
    sem_col = _pick_col(df, ["í•™ê¸°"])
    reading_col = _pick_col(df, ["ë…ì„œí™œë™ìƒí™©", "ë…ì„œí™œë™ ìƒí™©", "ë…ì„œí™œë™", "ë…ì„œ"])

    if not num_col:
        raise ValueError("ë…ì„œí™œë™ íŒŒì¼ì—ì„œ 'ë²ˆí˜¸' ì»¬ëŸ¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    if not sem_col:
        raise ValueError("ë…ì„œí™œë™ íŒŒì¼ì—ì„œ 'í•™ê¸°' ì»¬ëŸ¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    if not reading_col:
        raise ValueError("ë…ì„œí™œë™ íŒŒì¼ì—ì„œ 'ë…ì„œí™œë™ìƒí™©' ì»¬ëŸ¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

    if not year_col:
        inferred_year = _infer_academic_year_from_print_date(df_raw_top) or 2025
        df["__í•™ë…„ë„__"] = str(inferred_year)
        year_col = "__í•™ë…„ë„__"

    if not name_col:
        if last_col and first_col:
            df["__ì´ë¦„__"] = df[last_col].fillna("").astype(str).str.strip() + df[first_col].fillna("").astype(str).str.strip()
            name_col = "__ì´ë¦„__"
        else:
            raise ValueError("ë…ì„œí™œë™ íŒŒì¼ì—ì„œ 'ì„±ëª…/ì´ë¦„' ì»¬ëŸ¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

    df[num_col] = df[num_col].ffill()
    df[name_col] = df[name_col].ffill()
    df[year_col] = df[year_col].ffill()
    df[sem_col] = df[sem_col].ffill()

    df2 = df[[num_col, name_col, year_col, sem_col, reading_col]].copy()
    df2.columns = ["ë²ˆí˜¸", "ì´ë¦„", "í•™ë…„ë„", "í•™ê¸°", "ë…ì„œí™œë™ìƒí™©"]
    df2["ë²ˆí˜¸"] = df2["ë²ˆí˜¸"].astype(str).str.extract(r"(\d+)")[0]
    df2 = df2.dropna(subset=["ë²ˆí˜¸", "ì´ë¦„", "í•™ë…„ë„", "í•™ê¸°"]).copy()

    groups = []
    for (num, name, year, sem), g in df2.groupby(["ë²ˆí˜¸", "ì´ë¦„", "í•™ë…„ë„", "í•™ê¸°"], dropna=False):
        all_books: List[Tuple[str, str]] = []
        for cell in g["ë…ì„œí™œë™ìƒí™©"].tolist():
            all_books.extend(_split_books(cell))

        seen_in_group: List[dict] = []
        uniq_books: List[Tuple[str, str]] = []
        dup_in_group: List[Tuple[str, str, str]] = []

        for t, a in all_books:
            hit = _find_duplicate_against_seen(t, a, seen_in_group)
            if hit:
                it, ts, ascore = hit
                dup_in_group.append((t, a, f"ìœ ì‚¬ì¤‘ë³µ(ì œëª© {ts:.2f}, ì €ì {ascore:.2f})"))
                continue
            seen_in_group.append({"title": t, "author": a})
            uniq_books.append((t, a))

        if grade is not None and cls is not None:
            sid = f"{int(grade)}{int(cls):02d}{int(num):02d}"
        else:
            sid = ""

        groups.append(
            {
                "ë²ˆí˜¸": int(num),
                "ì´ë¦„": str(name).strip(),
                "í•™ë…„ë„": str(year).strip(),
                "í•™ê¸°": str(sem).strip(),
                "í•™ë²ˆ": sid,
                "_books_raw": uniq_books,
                "_dup_in_group": dup_in_group,
            }
        )

    groups_sorted = sorted(groups, key=lambda x: (x["í•™ë²ˆ"], _semester_sort_key(x["í•™ë…„ë„"], x["í•™ê¸°"]), x["ë²ˆí˜¸"]))
    seen_by_student: Dict[str, List[dict]] = {}

    output_rows = []
    for item in groups_sorted:
        sid = item["í•™ë²ˆ"]
        student_key = sid or f"NOID-{item['ë²ˆí˜¸']}-{item['ì´ë¦„']}"
        if student_key not in seen_by_student:
            seen_by_student[student_key] = []

        req_map = _choose_required_map(item["í•™ë…„ë„"], req_2024_map, req_2025_map)
        req_keys = set(req_map.keys())
        req_title_map = _build_required_title_map(req_map)

        included: List[Tuple[str, str]] = []
        dup_remarks: List[str] = []
        review_remarks: List[str] = []

        for (t, a, reason) in item["_dup_in_group"]:
            dup_remarks.append(f"{t}({a}) ì¤‘ë³µ[{reason}]")

        for (t, a) in item["_books_raw"]:
            hit = _find_duplicate_against_seen(t, a, seen_by_student[student_key])
            if hit:
                it, ts, ascore = hit
                dup_remarks.append(
                    f"{t}({a}) ì¤‘ë³µ[í•™ìƒì „ì²´ ìœ ì‚¬ì¤‘ë³µ: ì œëª© {ts:.2f}, ì €ì {ascore:.2f}] â†” {it.get('title','')}({it.get('author','')})"
                )
                continue
            seen_by_student[student_key].append({"title": t, "author": a})
            included.append((t, a))

        books_for_cell: List[Tuple[str, str, bool]] = []
        required_count = 0

        for (t, a) in included:
            nt = _normalize_text(t)
            na = _normalize_text(a)
            exact_key = nt + "|" + na

            if exact_key in req_keys:
                required_count += 1
                books_for_cell.append((t, a, True))
                continue

            matched_std = None
            for k in _title_variants_norm(t):
                if k in req_title_map:
                    matched_std = req_title_map[k]
                    break

            if matched_std:
                std_t, std_a = matched_std
                a_score = _best_author_similarity(a, std_a)
                if a_score >= REQUIRED_AUTHOR_THRESHOLD:
                    required_count += 1
                    books_for_cell.append((t, a, True))
                    if _normalize_text(a) != _normalize_text(std_a):
                        review_remarks.append(
                            f"í•„ë…ì„œ ì €ì í‘œê¸° ìƒì´(í™•ì •, ì €ì {a_score:.2f}): {t}({a}) â†’ í•„ë…: {std_t}({std_a})"
                        )
                    continue
                review_remarks.append(
                    f"í•„ë…ì„œ ì œëª© ì¼ì¹˜(ê²€í† , ì €ì {a_score:.2f}): {t}({a}) â†’ í›„ë³´: {std_t}({std_a})"
                )
                books_for_cell.append((t, a, False))
                continue

            fuzzy = _fuzzy_match_required_title_author(
                raw_title=t,
                raw_author=a,
                req_title_map=req_title_map,
                title_threshold=REQUIRED_TITLE_THRESHOLD,
                author_threshold=REQUIRED_AUTHOR_THRESHOLD,
            )
            if fuzzy:
                _, (std_t, std_a), tscore, ascore = fuzzy
                required_count += 1
                books_for_cell.append((t, a, True))
                review_remarks.append(
                    f"í•„ë…ì„œ ìœ ì‚¬(í™•ì •, ì œëª© {tscore:.2f}/ì €ì {ascore:.2f}): {t}({a}) â†’ ì¶”ì • í•„ë…: {std_t}({std_a})"
                )
                continue

            books_for_cell.append((t, a, False))

        total_count = len(included)
        satisfied = "ì¶©ì¡±" if (total_count >= 5 and required_count >= 1) else "ë¯¸ì¶©ì¡±"

        remarks_all = sorted(set(dup_remarks + review_remarks))
        output_rows.append(
            {
                "í•™ë²ˆ": sid,
                "ì´ë¦„": item["ì´ë¦„"],
                "í•™ë…„ë„": item["í•™ë…„ë„"],
                "í•™ê¸°": item["í•™ê¸°"],
                "ë„ì„œëª©ë¡_í‘œì‹œ": books_for_cell,
                "ì´ê¶Œìˆ˜": total_count,
                "í•„ë…ì„œ ê¶Œìˆ˜": required_count,
                "ì¶©ì¡± ì—¬ë¶€": satisfied,
                "ë¹„ê³ ": "\n".join(remarks_all),
            }
        )

    return {"gc_text": gc_text, "output_rows": output_rows}


if run:
    if not reading_file:
        st.error("ë…ì„œí™œë™ìƒí™© íŒŒì¼ì„ ì—…ë¡œë“œí•´ ì£¼ì„¸ìš”.")
        st.stop()

    prog = st.progress(0, text="ë¶„ì„ ì¤€ë¹„ ì¤‘...")
    try:
        with st.spinner("ë¶„ì„ì„ ì§„í–‰í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
            prog.progress(10, text="ì—‘ì…€ íŒŒì¼ ì½ëŠ” ì¤‘...")
            result = _analyze(reading_file)
            prog.progress(80, text="ê²°ê³¼ ì •ë¦¬ ì¤‘...")

        st.session_state.analysis = result
        st.session_state.analysis_id = current_id
        st.session_state.excel_bytes = None
        st.session_state.excel_name = None
        st.session_state.excel_id = None
        for k in ["cb_11", "cb_12", "cb_21", "cb_22", "cb_init_for_id"]:
            if k in st.session_state:
                del st.session_state[k]

        prog.progress(100, text="ì™„ë£Œ")
        st.success("ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ì•„ë˜ì—ì„œ í•™ê¸° ê¸°ì¤€ì„ ì¡°ì •í•´ ì£¼ì„¸ìš”.")
    except Exception as e:
        prog.empty()
        st.error(f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
        st.stop()

analysis = st.session_state.analysis
if analysis is None:
    st.stop()

output_rows: List[dict] = analysis["output_rows"]  # type: ignore
gc_text: str = analysis["gc_text"]  # type: ignore

# =========================================================
# 5) í•™ê¸° ì„ íƒ(ì²´í¬ í•´ì œ ì‹œ '-' ì²˜ë¦¬ + ìƒì„¸/ì—‘ì…€ë„ ë™ì¼ ë°˜ì˜)
# =========================================================
years_all = sorted({y for y in (_safe_year_int(r["í•™ë…„ë„"]) for r in output_rows) if y is not None})
base_year = years_all[0] if years_all else None
next_year = years_all[1] if len(years_all) >= 2 else None

available_keys: set = set()
for r in output_rows:
    k = _row_semkey(r, base_year, next_year)
    if k:
        available_keys.add(k)

init_key = st.session_state.analysis_id
if st.session_state.get("cb_init_for_id") != init_key:
    st.session_state["cb_11"] = ("11" in available_keys)
    st.session_state["cb_12"] = ("12" in available_keys)
    st.session_state["cb_21"] = ("21" in available_keys)
    st.session_state["cb_22"] = ("22" in available_keys)
    st.session_state["cb_init_for_id"] = init_key

st.subheader("ì´ ì¶©ì¡± ê¸°ì¤€(í•™ê¸°) ì„ íƒ")
st.caption("ì²´í¬ í•´ì œí•œ í•™ê¸°ëŠ” ìš”ì•½ì—ì„œ '-'ë¡œ í‘œì‹œë˜ë©°, ì´ ì¶©ì¡± ì—¬ë¶€ ê³„ì‚° ë° ìƒì„¸/ì—‘ì…€ì—ì„œ ì œì™¸ë©ë‹ˆë‹¤.")

colA, colB = st.columns(2)
with colA:
    st.checkbox("1í•™ë…„ 1í•™ê¸°", key="cb_11", disabled=("11" not in available_keys))
    st.checkbox("1í•™ë…„ 2í•™ê¸°", key="cb_12", disabled=("12" not in available_keys))
with colB:
    st.checkbox("2í•™ë…„ 1í•™ê¸°", key="cb_21", disabled=("21" not in available_keys))
    st.checkbox("2í•™ë…„ 2í•™ê¸°", key="cb_22", disabled=("22" not in available_keys))

selected_keys: List[str] = []
if st.session_state.get("cb_11"):
    selected_keys.append("11")
if st.session_state.get("cb_12"):
    selected_keys.append("12")
if st.session_state.get("cb_21"):
    selected_keys.append("21")
if st.session_state.get("cb_22"):
    selected_keys.append("22")

# ì„ íƒì´ ë°”ë€Œë©´(ë‹¤ìš´ë¡œë“œ íŒŒì¼ ë¶ˆì¼ì¹˜ ë°©ì§€) ìƒì„±ëœ ì—‘ì…€ ìºì‹œ ì œê±°
excel_id = f"{st.session_state.analysis_id}:{','.join(selected_keys)}"
if st.session_state.excel_id is not None and st.session_state.excel_id != excel_id:
    st.session_state.excel_bytes = None
    st.session_state.excel_name = None
    st.session_state.excel_id = None
    st.info("í•™ê¸° ì„ íƒì´ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤. ì—‘ì…€ íŒŒì¼ì€ ë‹¤ì‹œ ìƒì„±í•´ ì£¼ì„¸ìš”.")

if len(selected_keys) == 0:
    st.warning("ì„ íƒëœ í•™ê¸°ê°€ ì—†ìŠµë‹ˆë‹¤. ì´ ì¶©ì¡± ì—¬ë¶€ëŠ” 'íŒì • ë³´ë¥˜'ë¡œ í‘œì‹œë˜ë©°, ìƒì„¸/ì—‘ì…€ì€ ë¹„ì–´ ìˆê²Œ ë©ë‹ˆë‹¤.")

# =========================================================
# 6) ìš”ì•½ ìƒì„± (ì²´í¬ í•´ì œ í•™ê¸° = '-', ì„ íƒ í•™ê¸°ë§Œ O/X)
# =========================================================
def _default_value_for_sem(k: str) -> str:
    if k not in selected_keys:
        return "-"  # ì„ íƒ ì•ˆ í•¨
    return "X"  # ì„ íƒí•¨(íŒŒì¼ì— ì¡´ì¬í•˜ëŠ” í•™ê¸°ë§Œ ì„ íƒ ê°€ëŠ¥í•˜ë„ë¡ UIì—ì„œ ì œì–´)

summary_map: Dict[str, dict] = {}

for r in output_rows:
    sid = r["í•™ë²ˆ"]
    key = sid or f"NOID-{r['ì´ë¦„']}"
    if key not in summary_map:
        summary_map[key] = {
            "í•™ë²ˆ": sid,
            "ì´ë¦„": r["ì´ë¦„"],
            "11": _default_value_for_sem("11"),
            "12": _default_value_for_sem("12"),
            "21": _default_value_for_sem("21"),
            "22": _default_value_for_sem("22"),
        }

    semkey = _row_semkey(r, base_year, next_year)
    if not semkey or semkey not in selected_keys:
        continue  # ì„ íƒë˜ì§€ ì•Šì€ í•™ê¸°ëŠ” ê¸°ë¡ì´ ìˆì–´ë„ '-' ìœ ì§€

    mark = "O" if r["ì¶©ì¡± ì—¬ë¶€"] == "ì¶©ì¡±" else "X"
    summary_map[key][semkey] = mark

def _sort_key_summary(item: dict):
    sid = item.get("í•™ë²ˆ", "") or ""
    return (0, sid) if sid else (1, item.get("ì´ë¦„", ""))

summary_rows = sorted(summary_map.values(), key=_sort_key_summary)

summary_final = []
for i, row in enumerate(summary_rows, start=1):
    if len(selected_keys) == 0:
        total = "íŒì • ë³´ë¥˜"
    else:
        ok = True
        for k in selected_keys:
            if row.get(k, "-") != "O":
                ok = False
                break
        total = "ì¶©ì¡±" if ok else "ë¯¸ì¶©ì¡±"

    summary_final.append(
        {
            "ì—°ë²ˆ": i,
            "í•™ë²ˆ": row["í•™ë²ˆ"],
            "ì´ë¦„": row["ì´ë¦„"],
            "1í•™ë…„ 1í•™ê¸° ì¶©ì¡±ì—¬ë¶€": row["11"],
            "1í•™ë…„ 2í•™ê¸° ì¶©ì¡±ì—¬ë¶€": row["12"],
            "2í•™ë…„ 1í•™ê¸° ì¶©ì¡±ì—¬ë¶€": row["21"],
            "2í•™ë…„ 2í•™ê¸° ì¶©ì¡±ì—¬ë¶€": row["22"],
            "ì´ ì¶©ì¡±ì—¬ë¶€": total,
        }
    )

df_summary = pd.DataFrame(
    summary_final,
    columns=[
        "ì—°ë²ˆ",
        "í•™ë²ˆ",
        "ì´ë¦„",
        "1í•™ë…„ 1í•™ê¸° ì¶©ì¡±ì—¬ë¶€",
        "1í•™ë…„ 2í•™ê¸° ì¶©ì¡±ì—¬ë¶€",
        "2í•™ë…„ 1í•™ê¸° ì¶©ì¡±ì—¬ë¶€",
        "2í•™ë…„ 2í•™ê¸° ì¶©ì¡±ì—¬ë¶€",
        "ì´ ì¶©ì¡±ì—¬ë¶€",
    ],
)

# =========================================================
# 7) ìƒì„¸(ì„ íƒ í•™ê¸°ë§Œ ë‚¨ê¹€)
# =========================================================
detail_rows_selected = []
for r in output_rows:
    semkey = _row_semkey(r, base_year, next_year)
    if semkey and semkey in selected_keys:
        detail_rows_selected.append(r)

detail_rows_selected = sorted(detail_rows_selected, key=lambda x: (x.get("í•™ë²ˆ", ""), _semester_sort_key(x.get("í•™ë…„ë„", ""), x.get("í•™ê¸°", "")), x.get("ì´ë¦„", "")))

st.subheader("ìš”ì•½ ë¯¸ë¦¬ë³´ê¸°")
st.dataframe(df_summary.head(20), use_container_width=True)

st.subheader("ìƒì„¸ ë¯¸ë¦¬ë³´ê¸°(ìƒìœ„ 20í–‰)")
df_detail_preview = pd.DataFrame(
    [
        {
            "ì—°ë²ˆ": i,
            "í•™ë²ˆ": r["í•™ë²ˆ"],
            "ì´ë¦„": r["ì´ë¦„"],
            "í•™ë…„ë„": r["í•™ë…„ë„"],
            "í•™ê¸°": r["í•™ê¸°"],
            "ì´ê¶Œìˆ˜": r["ì´ê¶Œìˆ˜"],
            "í•„ë…ì„œ ê¶Œìˆ˜": r["í•„ë…ì„œ ê¶Œìˆ˜"],
            "ì¶©ì¡± ì—¬ë¶€": r["ì¶©ì¡± ì—¬ë¶€"],
            "ë¹„ê³ ": r["ë¹„ê³ "],
        }
        for i, r in enumerate(detail_rows_selected, start=1)
    ]
)
st.dataframe(df_detail_preview.head(20), use_container_width=True)

# =========================================================
# 8) ì—‘ì…€ ìƒì„±/ë‹¤ìš´ë¡œë“œ: ìƒì„± ë²„íŠ¼ + ìŠ¤í”¼ë„ˆ/ì§„í–‰ë°” ì œê³µ (ìš”ì²­ 2)
# =========================================================
def _build_excel_bytes(df_sum: pd.DataFrame, detail_rows: List[dict]) -> Tuple[bytes, str]:
    wb = Workbook()

    # ìš”ì•½
    ws_sum = wb.active
    ws_sum.title = "ìš”ì•½"
    sum_headers = list(df_sum.columns)
    ws_sum.append(sum_headers)
    for _, row in df_sum.iterrows():
        ws_sum.append([row[h] for h in sum_headers])

    header_font = Font(bold=True)
    for c in range(1, len(sum_headers) + 1):
        cell = ws_sum.cell(row=1, column=c)
        cell.font = header_font
        cell.alignment = Alignment(horizontal="center", vertical="center", wrap_text=True)

    ws_sum.column_dimensions["A"].width = 6
    ws_sum.column_dimensions["B"].width = 10
    ws_sum.column_dimensions["C"].width = 10
    ws_sum.column_dimensions["D"].width = 18
    ws_sum.column_dimensions["E"].width = 18
    ws_sum.column_dimensions["F"].width = 18
    ws_sum.column_dimensions["G"].width = 18
    ws_sum.column_dimensions["H"].width = 12

    # ìƒì„¸(ì„ íƒ í•™ê¸°ë§Œ)
    ws = wb.create_sheet(title="ìƒì„¸")
    detail_headers = ["ì—°ë²ˆ", "í•™ë²ˆ", "ì´ë¦„", "í•™ë…„ë„", "í•™ê¸°", "ë„ì„œëª…", "ì´ê¶Œìˆ˜", "í•„ë…ì„œ ê¶Œìˆ˜", "ì¶©ì¡± ì—¬ë¶€", "ë¹„ê³ "]
    ws.append(detail_headers)

    for idx, r in enumerate(detail_rows, start=2):
        ws.append(
            [
                idx - 1,
                r["í•™ë²ˆ"],
                r["ì´ë¦„"],
                r["í•™ë…„ë„"],
                r["í•™ê¸°"],
                "",
                r["ì´ê¶Œìˆ˜"],
                r["í•„ë…ì„œ ê¶Œìˆ˜"],
                r["ì¶©ì¡± ì—¬ë¶€"],
                r["ë¹„ê³ "],
            ]
        )
        _set_books_cell(ws, row_idx=idx, col_idx=6, books=r["ë„ì„œëª©ë¡_í‘œì‹œ"])

        note_cell = ws.cell(row=idx, column=10)
        note_cell.alignment = Alignment(wrap_text=True, vertical="top")
        if "ì¤‘ë³µ" in str(r.get("ë¹„ê³ ", "") or ""):
            note_cell.font = Font(color="FF0000")

    for c in range(1, len(detail_headers) + 1):
        cell = ws.cell(row=1, column=c)
        cell.font = header_font
        cell.alignment = Alignment(horizontal="center", vertical="center", wrap_text=True)

    ws.column_dimensions["A"].width = 6
    ws.column_dimensions["B"].width = 10
    ws.column_dimensions["C"].width = 10
    ws.column_dimensions["D"].width = 10
    ws.column_dimensions["E"].width = 8
    ws.column_dimensions["F"].width = 70
    ws.column_dimensions["G"].width = 8
    ws.column_dimensions["H"].width = 12
    ws.column_dimensions["I"].width = 10
    ws.column_dimensions["J"].width = 55

    for rr in range(2, ws.max_row + 1):
        ws.cell(row=rr, column=6).alignment = Alignment(wrap_text=True, vertical="top")

    out = BytesIO()
    wb.save(out)
    out.seek(0)

    sem_tag = "none" if len(selected_keys) == 0 else "-".join(selected_keys)
    filename = f"{gc_text}_ë…ì„œê°€_ê²°ê³¼_{sem_tag}.xlsx"
    return out.getvalue(), filename


st.divider()
st.subheader("ì—‘ì…€ ë‹¤ìš´ë¡œë“œ")

create_excel = st.button("ğŸ“¦ ì—‘ì…€ íŒŒì¼ ìƒì„±", use_container_width=True)
if create_excel:
    p = st.progress(0, text="ì—‘ì…€ ìƒì„± ì¤€ë¹„ ì¤‘...")
    with st.spinner("ì—‘ì…€ íŒŒì¼ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
        p.progress(20, text="ìš”ì•½ ì‹œíŠ¸ êµ¬ì„± ì¤‘...")
        p.progress(50, text="ìƒì„¸ ì‹œíŠ¸ êµ¬ì„± ì¤‘...")
        # (ì„ íƒ í•™ê¸°ë§Œ ë°˜ì˜)
        bytes_data, fname = _build_excel_bytes(df_summary, detail_rows_selected)
        p.progress(100, text="ì™„ë£Œ")
    st.session_state.excel_bytes = bytes_data
    st.session_state.excel_name = fname
    st.session_state.excel_id = excel_id
    st.success("ì—‘ì…€ íŒŒì¼ ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ì•„ë˜ ë²„íŠ¼ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œí•´ ì£¼ì„¸ìš”.")

if st.session_state.excel_bytes is not None:
    st.download_button(
        label="ğŸ“¥ ê²°ê³¼ ì—‘ì…€ ë‹¤ìš´ë¡œë“œ",
        data=st.session_state.excel_bytes,
        file_name=st.session_state.excel_name,
        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
        use_container_width=True,
    )

if not RICH_TEXT_AVAILABLE:
    st.warning("í˜„ì¬ í™˜ê²½ì—ì„œ 'ì…€ ë‚´ë¶€ ì¼ë¶€ êµµê²Œ'ê°€ ì œí•œë˜ì–´, í•„ë…ì„œëŠ” â˜… í‘œì‹œë¡œ ê°•ì¡°ë©ë‹ˆë‹¤.")
